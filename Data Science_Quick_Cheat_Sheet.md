# âš¡ QUICK REFERENCE CHEAT SHEET
## Last-Minute Exam Survival Guide

---

## ğŸ¯ DATA TYPES AT A GLANCE

### Numeric
- **Continuous**: Height, temperature, time (any value)
- **Discrete**: Clicks, counts, integers (specific values)

### Categorical
- **Nominal**: No order (colors, countries)
- **Ordinal**: Ranked (ratings, education level)
- **Binary**: Two values (yes/no, true/false)

---

## ğŸ“Š STATISTICS QUICK FACTS

### Location (Center)
- **Mean**: Average, sensitive to outliers
- **Median**: Middle value, robust
- **Trimmed Mean**: Remove extremes, middle ground

### Spread (Variability)
- **Variance/SD**: Around mean, sensitive outliers
- **IQR**: Q3 - Q1, robust
- **Percentile**: % of data below this

### Formulas
- **SD = âˆš(Î£(x-mean)Â²/n)**
- **IQR = Q3 - Q1**
- **SE = SD/âˆšn** (smaller = more precise)

---

## ğŸ”— CORRELATION QUICK REFERENCE

**Pearson's r**: -1 to +1
- **+1**: Perfect positive (both increase)
- **0**: No linear relationship
- **-1**: Perfect negative (opposite directions)
- **0.5 to 1 or -1 to -0.5**: Strong
- **0 to 0.3**: Weak

**Key**: Correlation â‰  Causation!

---

## ğŸ“ˆ SAMPLING CONCEPTS

| Concept | Meaning |
|---|---|
| **Random Sampling** | Each element equal chance (unbiased) |
| **Bias** | Systematic error in one direction |
| **Selection Bias** | Data collection skewed |
| **Regression to Mean** | Extremes followed by averages |

---

## ğŸ”” DISTRIBUTIONS CHEAT SHEET

| Distribution | Used For | Key Fact |
|---|---|---|
| **Normal** | General, baseline | Bell curve, mean=median |
| **t-Distribution** | Small samples | Fatter tails than normal |
| **Binomial** | Success/fail | Fixed trials, 2 outcomes |
| **Chi-Square** | Categorical | Testing independence |
| **F-Distribution** | Comparing variances | Always positive |
| **Poisson** | Event counts | Mean = Variance = Î» |

---

## â­ CENTRAL LIMIT THEOREM (CLT)

**The Magic**: Sample means â‰ˆ normal distribution
- Works even if population skewed!
- Bigger samples â†’ better approximation
- SE = Ïƒ/âˆšn (larger n â†’ smaller SE)

**Use**: Confidence intervals without knowing population distribution

---

## ğŸ² CONFIDENCE INTERVALS

**Formula**: XÌ„ Â± (z Ã— SE)

| Level | z-value |
|---|---|
| 90% | 1.645 |
| 95% | 1.96 |
| 99% | 2.58 |

**Interpretation**: NOT 95% chance value in interval, but 95% of such intervals contain true value!

---

## ğŸ”„ BOOTSTRAP IN 3 STEPS

1. Resample original data WITH replacement (many times)
2. Calculate statistic (mean, median) for each resample
3. Distribution of statistics â‰ˆ sampling distribution

**Why**: Works without assuming distribution, handles skewed data

---

## ğŸ¤– MACHINE LEARNING TYPES

### Supervised (Labeled Data)
- **Classification**: Predict categories
  - Binary: 2 classes
  - Multi-class: 3+ classes
  - Multi-label: Multiple per item
- **Regression**: Predict numbers

### Unsupervised (No Labels)
- **Clustering**: Group similar items

### Semi-Supervised
- Mix of labeled + unlabeled

### Reinforcement
- Learn from rewards/penalties

---

## ğŸ“ LINEAR REGRESSION

**Equation**: Y = mX + b

| Symbol | Meaning |
|---|---|
| **m** | Slope (change in Y per unit X) |
| **b** | Y-intercept (Y when X=0) |
| **mX+b** | Predicted value |
| **Actual - Predicted** | Residual/Error |

**Cost Function**: MSE = (1/2m)Î£(predicted-actual)Â²

**RÂ²**: % of variance explained (0 to 1, higher better)

---

## ğŸ¯ LOGISTIC REGRESSION

**Output**: Probability [0-1]

**Function**: Sigmoid Ïƒ(z) = 1/(1+e^(-z))

**Decision**: P â‰¥ 0.5 â†’ Class 1, else Class 0

**Key Difference**: Predicts probability, not continuous value!

**Cost**: Log Loss = -(1/m)Î£[yÃ—log(Å·) + (1-y)Ã—log(1-Å·)]

---

## ğŸ‘¥ K-NEAREST NEIGHBORS (KNN)

**Process**:
1. Choose k
2. Find k nearest points
3. Vote (classification) or average (regression)

**Distance Metrics**:
- **Euclidean**: d = âˆš[(xâ‚-xâ‚‚)Â² + (yâ‚-yâ‚‚)Â²]
- **Manhattan**: d = |xâ‚-xâ‚‚| + |yâ‚-yâ‚‚|

**Preprocessing**: Normalize (z-score), encode categorical (one-hot)

**k Choice**:
- Small k: Overfitting
- Large k: Underfitting
- Sweet spot: Cross-validation

---

## ğŸŒ³ DECISION TREES

**Split**: Minimize impurity

**Impurity Measures**:
- **Entropy**: 0 (pure) to 1 (50-50 mix)
- **Gini**: 0 (pure) to 0.5 (50-50 mix)

**Information Gain**: Reduction in impurity after split

**Stopping Rules**:
- Min samples per leaf
- Max depth
- Min information gain

**Regression**: Predict mean of values in leaf

---

## ğŸŒ² RANDOM FOREST

**Formula**: Bagging + Random Feature Selection

**Process**:
1. Bootstrap samples
2. Each tree: Random feature subset at each split
3. Aggregate (vote/average)

**vs Bagging**:
- RF: More diverse trees
- RF: Usually better accuracy
- Bagging: Simpler to understand

---

## ğŸ§  NAÃVE BAYES

**Core**: Bayes' Theorem = P(y|X) = P(X|y)Ã—P(y)/P(X)

**Assumption**: Features conditionally independent (naive, but works!)

**Types**:
- **Gaussian**: Continuous data
- **Multinomial**: Word counts (text)
- **Bernoulli**: Binary features (word present/absent)

---

## âœ… CLASSIFICATION METRICS

**Confusion Matrix**:
```
         Predicted
         Pos  Neg
Actual Pos [TP][FN]
       Neg [FP][TN]
```

| Metric | Formula | Meaning |
|---|---|---|
| **Accuracy** | (TP+TN)/Total | % correct (misleading if imbalanced!) |
| **Precision** | TP/(TP+FP) | Of predicted positives, % correct |
| **Recall** | TP/(TP+FN) | Of actual positives, % caught |
| **F1** | 2Ã—PÃ—R/(P+R) | Balanced (use for imbalanced) |

---

## ğŸª COMMON MISTAKES TO AVOID

1. **SD vs SE**: SD spreads data, SE spreads mean (SE = SD/âˆšn)
2. **Correlation = Causation**: FALSE! Can move together without causation
3. **CI Interpretation**: NOT 95% chance value in interval
4. **Accuracy with Imbalance**: Misleading! Use Precision/Recall/F1
5. **Overfitting**: High train acc, low test acc
6. **Underfitting**: High both errors

---

## ğŸ§® MUST-KNOW FORMULAS

```
Slope: m = Î£(x-xÌ„)(y-È³) / Î£(x-xÌ„)Â²
Intercept: b = È³ - mÃ—xÌ„
MSE: (1/2m)Î£(predicted-actual)Â²
SE: SD/âˆšn
Correlation: r = Î£(x-xÌ„)(y-È³) / âˆš[Î£(x-xÌ„)Â²Ã—Î£(y-È³)Â²]
CI: XÌ„ Â± zÃ—SE
IG: H(parent) - Î£(pÃ—H(child))
Gini: 1 - Î£(páµ¢Â²)
Precision: TP/(TP+FP)
Recall: TP/(TP+FN)
F1: 2Ã—PÃ—R/(P+R)
Sigmoid: Ïƒ(z) = 1/(1+e^(-z))
Bayes: P(y|X) = P(X|y)Ã—P(y)/P(X)
Euclidean: d = âˆš[Î£(xáµ¢-yáµ¢)Â²]
z-score: z = (x-mean)/SD
```

---

## âš¡ EXAM STRATEGY

**Time Management**
- ~2 min per mark
- Easy questions first
- Hardest last (if time permits)

**Answering Tips**
- Define terms before using
- Show all work (partial credit)
- Interpret results (don't just calculate)
- Check reasonableness
- Draw diagrams clearly

**What to Memorize**
- Formula list (above)
- Algorithm steps
- Metric interpretations
- Real-world applications

**What to Understand**
- WHY algorithms work
- When to use which
- Pros/cons of each
- Business context

---

## ğŸ“‹ CONCEPT QUICK CHECK (If unsure, review)

- [ ] Data types (numeric, categorical, nominal, ordinal, binary)
- [ ] Location vs Variability
- [ ] SD vs SE
- [ ] Correlation (range -1 to 1)
- [ ] Sampling bias & random sampling
- [ ] CLT & why important
- [ ] Bootstrap process
- [ ] Confidence interval interpretation
- [ ] ML types (supervised, unsupervised, semi-supervised)
- [ ] Linear regression (slope, intercept, MSE)
- [ ] Logistic regression (sigmoid, probability output)
- [ ] K-NN (process, k choice, distance metrics)
- [ ] Decision trees (splits, impurity, overfitting)
- [ ] Random Forest (bootstrap + random features)
- [ ] NaÃ¯ve Bayes (Bayes' theorem, conditional independence)
- [ ] Precision vs Recall vs F1
- [ ] Overfitting vs Underfitting

---

## ğŸ¯ HIGH-PROBABILITY EXAM QUESTIONS

1. **Calculate correlation** â†’ Use formula, interpret
2. **Build decision tree** â†’ Show splits, entropy/gini calculations
3. **Precision vs Recall** â†’ Define both, business context
4. **Linear regression** â†’ Calculate slope & intercept
5. **Confidence interval** â†’ Calculate, interpret correctly
6. **K-NN algorithm** â†’ Steps + preprocessing + k choice
7. **Logistic vs Linear** â†’ Key differences
8. **Random Forest vs Bagging** â†’ Compare
9. **Classification metrics** â†’ Confusion matrix, which metric when
10. **Bootstrap explanation** â†’ Why useful, how it works

---

**âœ¨ Remember: SHOW YOUR WORK. Good luck! ğŸ’ª**
