











# Final Answer

Below is a complete, examâ€ready system based on the provided course slides, organized into eight sections as requested.

---

## 1ï¸âƒ£ Complete Topic Map

â€¢ Exploratory Data Analysis  
  â€“ Elements of Structured Data  
    â€¢ Numeric: Continuous vs. Discrete  
    â€¢ Categorical: Nominal, Binary, Ordinal  
  â€“ Rectangular Data (Data Frames)  
    â€¢ Rows = records; Columns = features  
    â€¢ Indexes in databases vs. pandas  
  â€“ Nonrectangular Structures (time series, spatial, graph)  
  â€“ Estimates of Location  
    â€¢ Mean, Trimmed Mean, Weighted Mean  
    â€¢ Median and robustness to outliers  
  â€“ Estimates of Variability  
    â€¢ Variance, Standard Deviation, MAD  
    â€¢ Percentiles, IQR, Quantiles  
  â€“ Exploring Data Distribution  
    â€¢ Percentiles & Boxplots  
    â€¢ Frequency Tables & Histograms  
    â€¢ Density Plots  
  â€“ Binary & Categorical Data  
    â€¢ Mode, Expected Value, Bar Charts, Pie Charts  
  â€“ Correlation  
    â€¢ Pearsonâ€™s *r*, Correlation Matrix, Scatterplots, Heatmaps  
  â€“ Multivariate Exploration  
    â€¢ Hexagonal Binning, Contour Plots  
    â€¢ Boxplots & Violin Plots for categorical vs. numeric  
    â€¢ Contingency Tables  

â€¢ Data and Sampling Distributions  
  â€“ Random Sampling & Sample Bias  
    â€¢ Bias vs. random error; selfâ€selection bias  
  â€“ Sample Mean vs. Population Mean  
  â€“ Selection Bias & Regression to the Mean  
  â€“ Sampling Distribution of a Statistic  
  â€“ Central Limit Theorem & Standard Error  
  â€“ The Bootstrap (resampling with replacement)  
  â€“ Confidence Intervals  
  â€“ Key Distributions  
    â€¢ Normal, *t*, Binomial, Ï‡Â², *F*, Poisson  

â€¢ Machine Learning Fundamentals  
  â€“ Types of Learning  
    â€¢ Supervised (Regression, Classification)  
    â€¢ Unsupervised (Clustering)  
    â€¢ Semiâ€supervised, Selfâ€supervised, Reinforcement  
  â€“ Regression  
    â€¢ Simple Linear Regression: slope, intercept, cost function (MSE), gradient descent  
    â€¢ Multiple Linear Regression  
    â€¢ Prediction vs. Explanation (Profiling)  
    â€¢ RÂ², RMSE, MAE  
  â€“ Classification  
    â€¢ Logistic Regression: sigmoid, log loss, decision boundary, evaluation metrics  
    â€¢ NaÃ¯ve Bayes: Bayesâ€™ theorem, conditional independence assumption  
    â€¢ *k*â€Nearest Neighbors: distance metrics, choice of *k*, scaling, oneâ€hot encoding  
    â€¢ Decision Trees: recursive partitioning, impurity measures (Gini, entropy), pruning  
    â€¢ Ensemble Methods  
      â€“ Bagging (bootstrap aggregation)  
      â€“ Random Forest (bagging + feature randomness)  
      â€“ Variable Importance  

---

## 2ï¸âƒ£ Advanced Concept Explanations

### Elements of Structured Data  
â€¢ **What**: Data organized into rows (records) and columns (features).  
â€¢ **Why**: Enables statistical modeling, ML algorithms, and visualization.  
â€¢ **How**: Use pandas DataFrames in Python; tables in SQL.  
â€¢ **Mistake**: Treating highâ€cardinality categorical variables as numeric.  
â€¢ **Example**: Convert â€œGenderâ€ (Male/Female) into oneâ€hot columns, not integers 1/2.

### Estimates of Location  
â€¢ **Mean**: Sum of values divided by count. Sensitive to outliers.  
â€¢ **Median**: Middle value when sorted. Robust to outliers.  
â€¢ **Trimmed Mean**: Mean after removing top/bottom *p*%. Compromise between mean and median.  
â€¢ **Weighted Mean**: Mean where each value has a weight reflecting importance.  
â€¢ **Example**: In income data (rightâ€skewed), median often better represents â€œtypicalâ€ income.

### Estimates of Variability  
â€¢ **Variance**: Average squared deviation from the mean. UnitsÂ².  
â€¢ **Standard Deviation**: Square root of variance. Same units as data.  
â€¢ **MAD**: Median absolute deviation from median. Robust.  
â€¢ **IQR**: Difference between 75th and 25th percentiles. Robust range measure.  
â€¢ **Example**: For comparing spread of test scores across classes, use IQR if outliers present.

### Central Limit Theorem & Standard Error  
â€¢ **CLT**: Sampling distribution of the mean approaches normal as *n* increases, regardless of population distribution.  
â€¢ **Standard Error**: SD of sampling distribution = *s*/âˆš*n*. Measures precision of sample mean.  
â€¢ **Why**: Justifies confidence intervals and hypothesis tests for means.  
â€¢ **Mistake**: Applying CLT when *n* is too small (*n* < 30 rule of thumb).

### Bootstrap  
â€¢ **What**: Resample with replacement from observed data to approximate sampling distribution.  
â€¢ **Why**: Nonâ€parametric way to estimate SE, bias, and CI without distributional assumptions.  
â€¢ **How**: Draw *B* samples (e.g., 10,000) of size *n* with replacement; compute statistic for each.  
â€¢ **Example**: Estimate 95% CI for median by taking 2.5th and 97.5th percentiles of bootstrap medians.

### Logistic Regression  
â€¢ **What**: Predicts probability of binary class via sigmoid function.  
â€¢ **Why**: Linear regression can give probabilities outside [0,1]; logistic constrains to [0,1].  
â€¢ **How**: Sigmoid(*z*) = 1/(1 + eâ»á¶»), where *z* = *Î²â‚€* + *Î²â‚x*.  
â€¢ **Mistake**: Ignoring multicollinearity among features.

### *k*â€Nearest Neighbors  
â€¢ **What**: Classifies new point by majority vote of its *k* closest neighbors.  
â€¢ **Why**: Simple, instanceâ€based, no training phase.  
â€¢ **How**: Choose distance metric (Euclidean, Manhattan), scale features, choose *k* via crossâ€validation.  
â€¢ **Mistake**: Using *k*=1 leads to overfitting; large *k* can underfit.

### Decision Trees & Random Forest  
â€¢ **Decision Tree**: Splits data using rules to maximize purity (minimize Gini/entropy).  
â€¢ **Overfitting**: Trees can grow too deep; control via max depth, min samples per leaf.  
â€¢ **Random Forest**: Ensemble of trees trained on bootstrap samples and random feature subsets.  
â€¢ **Why**: Reduces variance without increasing bias; handles nonâ€linearities.

---

## 3ï¸âƒ£ Exam Weight Analysis

| Topic                                        | Weight      | Rationale                                                                 |
|----------------------------------------------|-------------|---------------------------------------------------------------------------|
| Exploratory Data Analysis                    | â­â­â­â­â­      | Foundation of all data science; heavy emphasis in slides                  |
| Estimates of Location & Variability          | â­â­â­â­â­      | Core descriptive statistics; always tested                               |
| Data & Sampling Distributions                | â­â­â­â­       | CLT, bootstrap, CI are fundamental inference concepts                     |
| Linear & Logistic Regression                 | â­â­â­â­â­      | Core supervised learning algorithms; detailed slides                       |
| *k*â€Nearest Neighbors                       | â­â­â­        | Important but simpler; focus on distance metrics and scaling              |
| Decision Trees & Random Forest               | â­â­â­â­       | Major classification tools; ensemble methods highly valued                 |
| NaÃ¯ve Bayes                                  | â­â­         | Covered less extensively; basic probability application                  |
| Correlation & Multivariate Visualization      | â­â­â­â­       | Essential for understanding relationships; practical focus               |

---

## 4ï¸âƒ£ Formula Intelligence Module

### Mean  
\[ \bar{x} = \frac{1}{n}\sum_{i=1}^{n} x_i \]  
â€¢ **Variables**: *xáµ¢* = individual values; *n* = sample size.  
â€¢ **Purpose**: Central tendency of symmetric data.  
â€¢ **When to use**: Data is numeric and roughly symmetric; no extreme outliers.  
â€¢ **Example**: {2,3,5,8} â†’ mean = 4.5

### Standard Deviation  
\[ s = \sqrt{\frac{1}{n-1}\sum_{i=1}^{n}(x_i - \bar{x})^2} \]  
â€¢ **Variables**: *xáµ¢* = values; *xÌ„* = sample mean; *n* = sample size.  
â€¢ **Purpose**: Measure of spread in same units as data.  
â€¢ **When to use**: Symmetric data; need spread in original units.  
â€¢ **Example**: {1,2,3,4,5} â†’ *s* â‰ˆ 1.58

### Standard Error  
\[ SE = \frac{s}{\sqrt{n}} \]  
â€¢ **Variables**: *s* = sample SD; *n* = sample size.  
â€¢ **Purpose**: Precision of sample mean estimate.  
â€¢ **When to use**: Constructing CI or hypothesis tests for mean.  
â€¢ **Example**: *s*=10, *n*=100 â†’ SE=1

### Simple Linear Regression  
\[ y = \beta_0 + \beta_1 x + \epsilon \]  
\[ \hat{\beta}_1 = \frac{\sum{(x_i - \bar{x})(y_i - \bar{y})}}{\sum{(x_i - \bar{x})^2}} \]  
\[ \hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x} \]  
â€¢ **Variables**: *x* = predictor; *y* = outcome; *Î²â‚€* = intercept; *Î²â‚* = slope.  
â€¢ **Purpose**: Model linear relationship; predict *y* from *x*.  
â€¢ **When to use**: Continuous outcome; linear relationship assumption holds.  
â€¢ **Example**: Hours studied vs. exam score.

### Logistic Regression (Sigmoid)  
\[ p = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x)}} \]  
â€¢ **Variables**: *x* = predictor; *Î²â‚€*, *Î²â‚* = coefficients; *p* = probability of class 1.  
â€¢ **Purpose**: Model probability of binary outcome.  
â€¢ **When to use**: Binary classification; interpretability needed.  
â€¢ **Example**: Probability of passing exam based on hours studied.

### Information Gain  
\[ IG(S, A) = \text{Entropy}(S) - \sum_{v \in A} \frac{|S_v|}{|S|} \text{Entropy}(S_v) \]  
â€¢ **Variables**: *S* = dataset; *A* = attribute to split on; *Sáµ¥* = subset where *A*=*v*.  
â€¢ **Purpose**: Choose best split in decision trees.  
â€¢ **When to use**: Building classification trees (ID3/C4.5).  
â€¢ **Example**: Splitting on â€œOutlookâ€ in Play Tennis dataset.

### Euclidean Distance  
\[ d(\mathbf{x}, \mathbf{y}) = \sqrt{\sum_{i=1}^{n}(x_i - y_i)^2} \]  
â€¢ **Variables**: *x*, *y* = two data points; *n* = number of features.  
â€¢ **Purpose**: Measure straightâ€line distance in feature space.  
â€¢ **When to use**: KNN, clustering algorithms with continuous features.  
â€¢ **Example**: Distance between (1,2) and (4,6) = 5.

---

## 5ï¸âƒ£ Smart Revision Notes (Compressed Version)

â€¢ **Structured Data Types**  
  â€“ Numeric: Continuous (any value) vs. Discrete (integers).  
  â€“ Categorical: Nominal (no order), Binary (2 values), Ordinal (ordered).  

â€¢ **Location Estimates**  
  â€“ Mean: sensitive to outliers.  
  â€“ Median: robust; middle value.  
  â€“ Trimmed Mean: remove extremes, then average.  

â€¢ **Variability Estimates**  
  â€“ SD: average spread; sensitive to outliers.  
  â€“ IQR: range of middle 50%; robust.  
  â€“ MAD: median of absolute deviations; robust.  

â€¢ **Distributions**  
  â€“ Normal: bellâ€shaped; mean=median=mode.  
  â€“ *t*-distribution: heavier tails for small samples.  
  â€“ CLT: sample mean â†’ normal as *n* â†’ âˆ.  

â€¢ **Sampling & Inference**  
  â€“ Bootstrap: resample with replacement to estimate SE/CI.  
  â€“ Confidence Interval: range likely to contain true parameter.  

â€¢ **Regression**  
  â€“ Linear: predict continuous *y*; minimize MSE.  
  â€“ Logistic: predict probability; use sigmoid; log loss cost.  

â€¢ **Classification Algorithms**  
  â€“ KNN: majority vote of *k* nearest; scale features.  
  â€“ Decision Tree: split by maximizing information gain; prune to avoid overfitting.  
  â€“ Random Forest: many trees on bootstrapped samples + random features.  

â€¢ **Evaluation**  
  â€“ Regression: RÂ², RMSE, MAE.  
  â€“ Classification: Accuracy, Precision, Recall, F1.  

â€¢ **Visualization**  
  â€“ Boxplot: median, quartiles, outliers.  
  â€“ Histogram: frequency distribution.  
  â€“ Scatterplot: relationship between two numeric variables.  
  â€“ Heatmap: correlation matrix.  

---

## 6ï¸âƒ£ Demo Exam Paper

### Multiple Choice (1 mark each)  
1. Which measure of central tendency is most robust to outliers?  
   a) Mean  
   b) Median  
   c) Mode  
   d) Trimmed mean  

2. The Central Limit Theorem states that:  
   a) The sample mean equals the population mean.  
   b) The sampling distribution of the mean approaches normal as *n* increases.  
   c) All data distributions are normal.  
   d) Variance decreases with sample size.  

3. In logistic regression, the sigmoid function:  
   a) Converts linear output to a probability between 0 and 1.  
   b) Minimizes squared error.  
   c) Is used only for multiâ€class classification.  
   d) Computes Euclidean distance.  

### Short Questions (3 marks each)  
1. Define and contrast Type I and Type II errors in hypothesis testing.  
2. Explain why we standardize features before applying KNN.  
3. What is information gain? How is it used in decision trees?  

### Numerical Problems (5 marks each)  
1. Given data {2, 4, 4, 4, 5, 5, 7, 9}:  
   a) Calculate the mean and median.  
   b) Compute the sample standard deviation.  
   c) Find the IQR.  

2. For a simple linear regression with predictor *x* = [1, 2, 3, 4] and response *y* = [2, 3, 5, 4]:  
   a) Calculate the slope (*Î²â‚*) and intercept (*Î²â‚€*).  
   b) Predict *y* when *x* = 5.  

### Long Questions (10 marks each)  
1. Explain the bootstrap method and how it can be used to construct a 95% confidence interval for the median.  
2. Compare and contrast bagging and Random Forest. How does Random Forest improve upon bagging?  

---

## 7ï¸âƒ£ Demo Exam Answer Key

### Multiple Choice  
1. b) Median  
2. b) The sampling distribution of the mean approaches normal as *n* increases.  
3. a) Converts linear output to a probability between 0 and 1.  

### Short Questions  
1. Type I: Reject true Hâ‚€ (false positive). Type II: Fail to reject false Hâ‚€ (false negative).  
2. Standardization (zâ€scores) puts all features on same scale; prevents highâ€magnitude features from dominating distance calculations in KNN.  
3. Information Gain = Entropy(parent) âˆ’ weighted Entropy(children). Used to select the best split in decision trees by maximizing IG.  

### Numerical Problems  
1. a) Mean = 5; Median = 4.5  
   b) *s* â‰ˆ 2.14  
   c) Q1 = 4, Q3 = 5.5 â†’ IQR = 1.5  
2. a) *Î²â‚* = 0.8; *Î²â‚€* = 1.5 â†’ *Å·* = 1.5 + 0.8*x*  
   b) When *x* = 5, *Å·* = 5.5  

### Long Questions  
1. **Bootstrap Method**:  
   â€“ Resample with replacement *B* times (e.g., 10,000) from the original sample.  
   â€“ Compute the median for each bootstrap sample.  
   â€“ The 95% CI is the 2.5th to 97.5th percentile of bootstrap medians.  
   â€“ Advantage: No assumption about population distribution; robust for skewed data.  

2. **Bagging vs. Random Forest**:  
   â€“ Bagging: Train each tree on a bootstrap sample of data; aggregate predictions (vote/average). Reduces variance.  
   â€“ Random Forest: Bagging + at each split, consider only a random subset of features. Further decorrelates trees, often leading to better generalization.  

---

## 8ï¸âƒ£ Final Output Format

The above sections together form a complete examâ€ready system covering all slide content, enriched with explanations, formulas, revision aids, and practice questions. Use the topic map for navigation, deep explanations for understanding, weight analysis to prioritize study, formula module for quick reference, revision notes for lastâ€minute review, and the demo exam for selfâ€assessment.

---

# âš¡ QUICK REFERENCE CHEAT SHEET
## Last-Minute Exam Survival Guide

---

## ğŸ¯ DATA TYPES AT A GLANCE

### Numeric
- **Continuous**: Height, temperature, time (any value)
- **Discrete**: Clicks, counts, integers (specific values)

### Categorical
- **Nominal**: No order (colors, countries)
- **Ordinal**: Ranked (ratings, education level)
- **Binary**: Two values (yes/no, true/false)

---

## ğŸ“Š STATISTICS QUICK FACTS

### Location (Center)
- **Mean**: Average, sensitive to outliers
- **Median**: Middle value, robust
- **Trimmed Mean**: Remove extremes, middle ground

### Spread (Variability)
- **Variance/SD**: Around mean, sensitive outliers
- **IQR**: Q3 - Q1, robust
- **Percentile**: % of data below this

### Formulas
- **SD = âˆš(Î£(x-mean)Â²/n)**
- **IQR = Q3 - Q1**
- **SE = SD/âˆšn** (smaller = more precise)

---

## ğŸ”— CORRELATION QUICK REFERENCE

**Pearson's r**: -1 to +1
- **+1**: Perfect positive (both increase)
- **0**: No linear relationship
- **-1**: Perfect negative (opposite directions)
- **0.5 to 1 or -1 to -0.5**: Strong
- **0 to 0.3**: Weak

**Key**: Correlation â‰  Causation!

---

## ğŸ“ˆ SAMPLING CONCEPTS

| Concept | Meaning |
|---|---|
| **Random Sampling** | Each element equal chance (unbiased) |
| **Bias** | Systematic error in one direction |
| **Selection Bias** | Data collection skewed |
| **Regression to Mean** | Extremes followed by averages |

---

## ğŸ”” DISTRIBUTIONS CHEAT SHEET

| Distribution | Used For | Key Fact |
|---|---|---|
| **Normal** | General, baseline | Bell curve, mean=median |
| **t-Distribution** | Small samples | Fatter tails than normal |
| **Binomial** | Success/fail | Fixed trials, 2 outcomes |
| **Chi-Square** | Categorical | Testing independence |
| **F-Distribution** | Comparing variances | Always positive |
| **Poisson** | Event counts | Mean = Variance = Î» |

---

## â­ CENTRAL LIMIT THEOREM (CLT)

**The Magic**: Sample means â‰ˆ normal distribution
- Works even if population skewed!
- Bigger samples â†’ better approximation
- SE = Ïƒ/âˆšn (larger n â†’ smaller SE)

**Use**: Confidence intervals without knowing population distribution

---

## ğŸ² CONFIDENCE INTERVALS

**Formula**: XÌ„ Â± (z Ã— SE)

| Level | z-value |
|---|---|
| 90% | 1.645 |
| 95% | 1.96 |
| 99% | 2.58 |

**Interpretation**: NOT 95% chance value in interval, but 95% of such intervals contain true value!

---

## ğŸ”„ BOOTSTRAP IN 3 STEPS

1. Resample original data WITH replacement (many times)
2. Calculate statistic (mean, median) for each resample
3. Distribution of statistics â‰ˆ sampling distribution

**Why**: Works without assuming distribution, handles skewed data

---

## ğŸ¤– MACHINE LEARNING TYPES

### Supervised (Labeled Data)
- **Classification**: Predict categories
  - Binary: 2 classes
  - Multi-class: 3+ classes
  - Multi-label: Multiple per item
- **Regression**: Predict numbers

### Unsupervised (No Labels)
- **Clustering**: Group similar items

### Semi-Supervised
- Mix of labeled + unlabeled

### Reinforcement
- Learn from rewards/penalties

---

## ğŸ“ LINEAR REGRESSION

**Equation**: Y = mX + b

| Symbol | Meaning |
|---|---|
| **m** | Slope (change in Y per unit X) |
| **b** | Y-intercept (Y when X=0) |
| **mX+b** | Predicted value |
| **Actual - Predicted** | Residual/Error |

**Cost Function**: MSE = (1/2m)Î£(predicted-actual)Â²

**RÂ²**: % of variance explained (0 to 1, higher better)

---

## ğŸ¯ LOGISTIC REGRESSION

**Output**: Probability [0-1]

**Function**: Sigmoid Ïƒ(z) = 1/(1+e^(-z))

**Decision**: P â‰¥ 0.5 â†’ Class 1, else Class 0

**Key Difference**: Predicts probability, not continuous value!

**Cost**: Log Loss = -(1/m)Î£[yÃ—log(Å·) + (1-y)Ã—log(1-Å·)]

---

## ğŸ‘¥ K-NEAREST NEIGHBORS (KNN)

**Process**:
1. Choose k
2. Find k nearest points
3. Vote (classification) or average (regression)

**Distance Metrics**:
- **Euclidean**: d = âˆš[(xâ‚-xâ‚‚)Â² + (yâ‚-yâ‚‚)Â²]
- **Manhattan**: d = |xâ‚-xâ‚‚| + |yâ‚-yâ‚‚|

**Preprocessing**: Normalize (z-score), encode categorical (one-hot)

**k Choice**:
- Small k: Overfitting
- Large k: Underfitting
- Sweet spot: Cross-validation

---

## ğŸŒ³ DECISION TREES

**Split**: Minimize impurity

**Impurity Measures**:
- **Entropy**: 0 (pure) to 1 (50-50 mix)
- **Gini**: 0 (pure) to 0.5 (50-50 mix)

**Information Gain**: Reduction in impurity after split

**Stopping Rules**:
- Min samples per leaf
- Max depth
- Min information gain

**Regression**: Predict mean of values in leaf

---

## ğŸŒ² RANDOM FOREST

**Formula**: Bagging + Random Feature Selection

**Process**:
1. Bootstrap samples
2. Each tree: Random feature subset at each split
3. Aggregate (vote/average)

**vs Bagging**:
- RF: More diverse trees
- RF: Usually better accuracy
- Bagging: Simpler to understand

---

## ğŸ§  NAÃVE BAYES

**Core**: Bayes' Theorem = P(y|X) = P(X|y)Ã—P(y)/P(X)

**Assumption**: Features conditionally independent (naive, but works!)

**Types**:
- **Gaussian**: Continuous data
- **Multinomial**: Word counts (text)
- **Bernoulli**: Binary features (word present/absent)

---

## âœ… CLASSIFICATION METRICS

**Confusion Matrix**:
```
         Predicted
         Pos  Neg
Actual Pos [TP][FN]
       Neg [FP][TN]
```

| Metric | Formula | Meaning |
|---|---|---|
| **Accuracy** | (TP+TN)/Total | % correct (misleading if imbalanced!) |
| **Precision** | TP/(TP+FP) | Of predicted positives, % correct |
| **Recall** | TP/(TP+FN) | Of actual positives, % caught |
| **F1** | 2Ã—PÃ—R/(P+R) | Balanced (use for imbalanced) |

---

## ğŸª COMMON MISTAKES TO AVOID

1. **SD vs SE**: SD spreads data, SE spreads mean (SE = SD/âˆšn)
2. **Correlation = Causation**: FALSE! Can move together without causation
3. **CI Interpretation**: NOT 95% chance value in interval
4. **Accuracy with Imbalance**: Misleading! Use Precision/Recall/F1
5. **Overfitting**: High train acc, low test acc
6. **Underfitting**: High both errors

---

## ğŸ§® MUST-KNOW FORMULAS

```
Slope: m = Î£(x-xÌ„)(y-È³) / Î£(x-xÌ„)Â²
Intercept: b = È³ - mÃ—xÌ„
MSE: (1/2m)Î£(predicted-actual)Â²
SE: SD/âˆšn
Correlation: r = Î£(x-xÌ„)(y-È³) / âˆš[Î£(x-xÌ„)Â²Ã—Î£(y-È³)Â²]
CI: XÌ„ Â± zÃ—SE
IG: H(parent) - Î£(pÃ—H(child))
Gini: 1 - Î£(páµ¢Â²)
Precision: TP/(TP+FP)
Recall: TP/(TP+FN)
F1: 2Ã—PÃ—R/(P+R)
Sigmoid: Ïƒ(z) = 1/(1+e^(-z))
Bayes: P(y|X) = P(X|y)Ã—P(y)/P(X)
Euclidean: d = âˆš[Î£(xáµ¢-yáµ¢)Â²]
z-score: z = (x-mean)/SD
```

---

## âš¡ EXAM STRATEGY

**Time Management**
- ~2 min per mark
- Easy questions first
- Hardest last (if time permits)

**Answering Tips**
- Define terms before using
- Show all work (partial credit)
- Interpret results (don't just calculate)
- Check reasonableness
- Draw diagrams clearly

**What to Memorize**
- Formula list (above)
- Algorithm steps
- Metric interpretations
- Real-world applications

**What to Understand**
- WHY algorithms work
- When to use which
- Pros/cons of each
- Business context

---

## ğŸ“‹ CONCEPT QUICK CHECK (If unsure, review)

- [ ] Data types (numeric, categorical, nominal, ordinal, binary)
- [ ] Location vs Variability
- [ ] SD vs SE
- [ ] Correlation (range -1 to 1)
- [ ] Sampling bias & random sampling
- [ ] CLT & why important
- [ ] Bootstrap process
- [ ] Confidence interval interpretation
- [ ] ML types (supervised, unsupervised, semi-supervised)
- [ ] Linear regression (slope, intercept, MSE)
- [ ] Logistic regression (sigmoid, probability output)
- [ ] K-NN (process, k choice, distance metrics)
- [ ] Decision trees (splits, impurity, overfitting)
- [ ] Random Forest (bootstrap + random features)
- [ ] NaÃ¯ve Bayes (Bayes' theorem, conditional independence)
- [ ] Precision vs Recall vs F1
- [ ] Overfitting vs Underfitting

---

## ğŸ¯ HIGH-PROBABILITY EXAM QUESTIONS

1. **Calculate correlation** â†’ Use formula, interpret
2. **Build decision tree** â†’ Show splits, entropy/gini calculations
3. **Precision vs Recall** â†’ Define both, business context
4. **Linear regression** â†’ Calculate slope & intercept
5. **Confidence interval** â†’ Calculate, interpret correctly
6. **K-NN algorithm** â†’ Steps + preprocessing + k choice
7. **Logistic vs Linear** â†’ Key differences
8. **Random Forest vs Bagging** â†’ Compare
9. **Classification metrics** â†’ Confusion matrix, which metric when
10. **Bootstrap explanation** â†’ Why useful, how it works

---

**âœ¨ Remember: SHOW YOUR WORK. Good luck! ğŸ’ª**
